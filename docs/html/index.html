<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Shield Perception: shield_perception</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Shield Perception
   &#160;<span id="projectnumber">v1.0</span>
   </div>
   <div id="projectbrief">The perception module for detecting, tracking and predicting the object to be intercepted by the Shield Planner</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">shield_perception </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> </p>
<h1><a class="anchor" id="autotoc_md1"></a>
Dependencies</h1>
<p><a href="https://www.stereolabs.com/developers/release">ZED SDK</a>\ <a href="https://wiki.ros.org/">ROS</a> packages</p>
<h1><a class="anchor" id="autotoc_md2"></a>
Hardware</h1>
<p>StereoLabs ZED 2i Stereo Camera <a href="https://www.stereolabs.com/products/zed-2">[link]</a></p>
<h1><a class="anchor" id="autotoc_md3"></a>
Calibration</h1>
<p><b>Requirements:</b> 3D print of camera calibration tool provided in <code>scripts/camera_calibration/calibration_tool</code></p>
<p><img src="./img/calib_tool.jpg" alt="" width="35%" height="35%" class="inline"/></p>
<p>We have an automated Hand-Eye calibration tool that finds the pose of the camera frame with respect to the base frame of the robot given an approximate initial pose. Using this approximate initial pose, we compute a Region of Interest (RoI). We use MuJoCo (included in this package) to model this RoI, sample poses from it. We then run IK to find a robot configuration to reach this pose such that the calibration tool is in the field of view of the camera and use a interpolator to command the robot to reach the configration. Once reached, we use the ZED SDK to save an image of the checkerboard pattern in the calibration tool. This data collected is used in the subsequent registration algorithm to find the pose of the camera frame from the base link.</p>
<p>To collect data for calibration, run </p><div class="fragment"><div class="line">cd scripts</div>
<div class="line">python camcalib.py</div>
</div><!-- fragment --><p> The collected data looks like <img src="./img/calib_data.png" alt="" width="100%" class="inline"/></p>
<p>To perform registration, run </p><div class="fragment"><div class="line">cd scripts</div>
<div class="line">python calibdata.py</div>
</div><!-- fragment --><p><b>Note:</b> The interpolator used above for calibration does not perform collision checking!</p>
<h1><a class="anchor" id="autotoc_md4"></a>
Perception Pipeline</h1>
<p>The perception pipeline that runs online consists of two parts</p><ol type="1">
<li>The detection submodule which continuously scans the environment to detect an object being thrown at the robot</li>
<li>The prediction submodule that predicts the future motion of the object given a set of detected poses.</li>
</ol>
<p><img src="./img/shield_percep_pipeline.png" alt="" width="35%" height="35%" class="inline"/></p>
<h2><a class="anchor" id="autotoc_md5"></a>
Ball Detection</h2>
<p>The pipeline assumes a particular color of the ball and performs color filtering over every dataframe of the camera to detect the ball. If there is any activity, we retrieve their depth values from the pointcloud and check for outliers and reject them to ensure that no spurious activity is detected. This camera is set at 60fps @ 720p.</p>
<h2><a class="anchor" id="autotoc_md6"></a>
Ball Prediction</h2>
<p>For prediction, we use a simple parabolic projectile model assuming no air drag. To find the predicted trajectory of the ball, we run an optimization to minimize the Least Means Square (LMS) of the error between the observed data and the model.</p>
<p><img src="./img/pred.png" alt="" width="75%" class="inline"/></p>
<p>To run the Perception Pipeline, first make sure that the <code>shield_planner</code> is running and connected to the robot and then run </p><div class="fragment"><div class="line">cd scripts</div>
<div class="line">python track_ball_sdk.py</div>
</div><!-- fragment --><p>The following parameters in the script needs to be tuned for every environment </p><div class="fragment"><div class="line"># Macro: </div>
<div class="line"># METHOD_ID: 0 = bounding box, 1 = color filtering</div>
<div class="line"># NUM_FRAME: number of frames required to start estimation</div>
<div class="line"># COLOR_FILTER: sets of color filtering in pc</div>
<div class="line">## 0: nothing</div>
<div class="line">## 1: lower = (160, 40, 40) upper = (250, 125, 125) - working, well?</div>
<div class="line"># PRINT_COLOR: print out color&#39;s bound</div>
<div class="line"># OUTLIER_REJECT: perform outlier reject or not</div>
<div class="line"># PASSTHROUGH: Filter data (turn off perception) outside a virtual bounding box</div>
<div class="line">METHOD_ID=0</div>
<div class="line">NUM_FRAME=3</div>
<div class="line">COLOR_FILTER=0</div>
<div class="line">PRINT_COLOR=0</div>
<div class="line">OUTLIER_REJECT=1</div>
<div class="line">PASSTHROUGH = 0</div>
</div><!-- fragment --> </div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
